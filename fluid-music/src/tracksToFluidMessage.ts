const fluid = require('./fluid/index');
const tab   = require('./tab');

import { FluidPlugin } from './plugin';
import { FluidTrack } from './FluidTrack';
import { TracksObject, ClipEventContext } from './ts-types';

/**
 * Create a `FluidMessage` from a `TracksObject`
 *
 * ```javascript
 * const session = fluid.score.parse(myScore, myConfig);
 * const message = fluid.score.tracksToFluidMessage(session.tracks);
 * const client = new fluid.Client();
 * client.send(message);
 * ```
 *
 * @param {TracksObject} tracksObject A tracks object generated by score.parse
 * @returns {FluidMessage}
 */
export function tracksToFluidMessage(tracks : FluidTrack[]) {
  const sessionMessages : any[] = [];
  let i = 0;

  // // example tracks object
  // const tracks = {
  //   bass: { clips: [ clip1, clip2... ] },
  //   kick: { clips: [ clip1, clip2... ] },
  // };
  for (const track of tracks) {
    if (tab.reservedKeys.hasOwnProperty(track.name)) {
      continue;
    }

    // Charles: This was buggy, and probably undesirable. After the typescript
    // refactor remove it altogether.
    // if (!track.clips || !track.clips.length) {
    //   if (!tracksObject.plugins.length) {
    //     console.log(`tracksToFluidMessage: skipping ${trackName}, because it has no .clips and no .plugins`);
    //     continue;
    //   }
    // }

    // Create a sub-message for each track
    let trackMessages : any[] = [];
    trackMessages.push(fluid.audiotrack.select(track.name));
    sessionMessages.push(trackMessages);

    track.clips.forEach((clip, clipIndex) => {

      // Create a sub-message for each clip. Note that the naming convention
      // gets a little confusing, because we do not yet know if "clip" contains
      // a single "Midi Clip", a collection of audio file events, or both.
      const clipMessages : any[] = [];
      trackMessages.push(clipMessages);

      // Create one EventContext object for each clip.
      const context : ClipEventContext = {
        track,
        clip,
        clipIndex,
        messages: clipMessages,
        data: {},
      };

      if (clip.midiEvents && clip.midiEvents.length) {
        clipMessages.push(midiEventsToFluidMessage(clip.midiEvents, context));
      }

      if (clip.fileEvents && clip.fileEvents.length) {
        clipMessages.push(fileEventsToFluidMessage(clip.fileEvents, context));
      }

    }); // track.clips.forEach

    // Handle track specific automation.
    for (const [name, automation] of Object.entries(track.automation)) {
      let trackAutoMsg : any[] = [];
      trackMessages.push(trackAutoMsg);
      if (name === 'volume' || name === 'pan') {
        // fluid.audiotrack.gain should always adjust the last volume plugin on
        // the track. That means that we want to apply automation on an earlier
        // volume plugin. First, ensure that we have at least two volume plugins
        trackAutoMsg.push(fluid.plugin.select('volume', 'tracktion', 1));
        // ...then select the first volume plugin which will be automated
        trackAutoMsg.push(fluid.plugin.select('volume', 'tracktion', 0));

        // Iterate over the automation points. If we are just dealing with
        // volume and pan, then the autoPoint should usable unedited. When
        // dealing with sends, this might need to be more complicated.
        for (const autoPoint of automation.points) {
          trackAutoMsg.push(createFluidMessageForAutomationPoint(name, autoPoint));
        }

      } else {
        throw new Error(`Fluid Track Automation found unsupported parameter: "${name}"`);
      }
    } // for [name, automation] of track.automation

    // Handle plugins/plugin automation
    const count : any = {};
    const nth = (plugin : FluidPlugin) => {
      const str = plugin.pluginName + '|' + plugin.pluginType;
      if (count.hasOwnProperty(str)) return count[str] = -1;
      return count[str]++;
    }
    for (const plugin of track.plugins) {
      trackMessages.push(fluid.plugin.select(plugin.pluginName, plugin.pluginType, nth(plugin)));
      for (const [paramName, automation] of Object.entries(plugin.automation)) {
        for (const autoPoint of automation.points) {
          trackMessages.push(createFluidMessageForAutomationPoint(paramName, autoPoint));
        } // for (autoPoint of automation.points)
      }   // for (paramName, automation of plugin.automation)
    }     // for (plugin of track.plugins)
  }       // for (track of tracks)

  return sessionMessages;
};

/**
 * FluidMessage objects are guaranteed to have either a .normalizedValue or a
 * .explicitValue -- this just checks which one the AutomationPoint has, and
 * returns an appropriate FluidMessage.
 *
 * Throws if the AutomationPoint does not have either type of value.
 *
 * This function is only used inside of tracksToFluidMessage, and should not be
 * exported.
 *
 * @param {string} paramName
 * @param {AutomationPoint} autoPoint
 * @returns {FluidMessage}
 */
const createFluidMessageForAutomationPoint = (paramName, autoPoint) => {
  if (typeof autoPoint.explicitValue === 'number') {
    return fluid.plugin.setParamExplicitAt(
      paramName,
      autoPoint.explicitValue,
      autoPoint.startTime,
      autoPoint.curve);
  } else if (typeof autoPoint.normalizedValue === 'number') {
    return fluid.plugin.setParamNormalizedAt(
      paramName,
      autoPoint.normalizedValue,
      autoPoint.startTime,
      autoPoint.curve);
  } else {
    throw new Error(`AutomationPoint has neither of .explicitValue/.normalizedValue: ${JSON.stringify(autoPoint)}`);
  }
}


/**
 * @param {ClipEvent[]} midiEvents
 * @param {ClipEventContext} context This will not have a .eventIndex
 */
function midiEventsToFluidMessage(midiEvents, context) {
  if (typeof context.clip.startTime !== 'number')
    throw new Error('Clip is missing startTime');

  const msg : any[] = [];
  const clipName  = `${context.track.name} ${context.clipIndex}`
  const startTime = context.clip.startTime;
  const duration  = context.clip.duration;
  const clipMsg   = fluid.midiclip.select(clipName, startTime, duration)
  msg.push(clipMsg);

  for (const event of midiEvents) {
    if (event.type === 'midiNote') {
      let velocity = (event.d && typeof event.d.v === 'number')
        ? event.d.v
        : (typeof event.v === 'number')
          ? event.v
          : undefined;
      msg.push(fluid.midiclip.note(event.n, event.startTime, event.duration, velocity));
    }
  }

  return msg;
};

/**
 * @param {ClipEvent[]} fileEvents
 * @param {ClipEventContext} context This will not have a .eventIndex
 */
function fileEventsToFluidMessage(fileEvents, context) {
  if (typeof context.clip.startTime !== 'number')
    throw new Error('Clip is missing startTime');

  // exampleClipEvent = {
  //   type: 'file',
  //   path: 'media/kick.wav',
  //   startTime: 0.50,
  //   length: 0.25,
  //   d: { v: 70, dbfs: -10 }, // If .v is present here...
  // };

  return fileEvents.map((event, eventIndex) => {
    const startTime = context.clip.startTime + event.startTime;

    if (typeof event.path !== 'string') {
      console.error(event);
      throw new Error('tracksToFluidMessage: A file event found in the note library does not have a .path string');
    };

    const clipName = `s${context.clipIndex}.${eventIndex}`;
    const msg = [fluid.audiotrack.insertWav(clipName, startTime, event.path)];

    if (event.startInSourceSeconds)
      msg.push(fluid.clip.setSourceOffsetSeconds(event.startInSourceSeconds));

    // adjust the clip length, unless the event is a .oneShot
    if (!event.oneShot)
      msg.push(fluid.clip.length(event.duration));

    // apply fade in/out times (if specified)
    if (typeof event.fadeOutSeconds === 'number' || typeof event.fadeInSeconds === 'number')
      msg.push(fluid.audioclip.fadeInOutSeconds(event.fadeInSeconds, event.fadeOutSeconds));

    // If there is a dynamics object, look for a dbfs property and apply gain.
    if (event.d && typeof(event.d.dbfs) === 'number')
      msg.push(fluid.audioclip.gain(event.d.dbfs));

    return msg;
  });
}

